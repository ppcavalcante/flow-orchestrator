# Benchmarks

This guide provides analysis of performance benchmarks for Flow Orchestrator, helping you understand the performance characteristics of different components and configurations.

## Overview

Flow Orchestrator's benchmark suite evaluates several critical aspects of the system's performance:

1. **Core component performance**: DAG construction, workflow data operations, node status management
2. **Memory optimization**: Arena allocation, string interning, memory pooling
3. **Concurrency and scalability**: Parallel execution, worker pool efficiency
4. **Real-world scenarios**: E-commerce, ETL processing, API orchestration

The benchmarks are designed to measure both micro-level performance (individual operations) and macro-level performance (end-to-end workflows) under various conditions and scales.

## Benchmark Categories

The benchmark suite is organized into several categories:

### Core Component Benchmarks

- **DAG Construction**: Measures the performance of creating DAGs with different topologies and sizes
- **Workflow Data**: Tests operations on the `WorkflowData` structure with different configurations
- **Node Status**: Evaluates the performance of node status tracking

### Memory Optimization Benchmarks

- **Arena Allocation**: Compares standard Go allocation with arena-based allocation
- **String Interning**: Measures the performance of string interning techniques
- **Memory Pooling**: Evaluates object pooling for nodes and other structures

### Concurrency Benchmarks

- **Parallel Execution**: Tests execution with varying worker counts
- **Concurrency Scaling**: Measures how performance scales with increased parallelism
- **Lock Contention**: Evaluates the impact of lock contention on performance

### Scalability Benchmarks

- **Node Scaling**: Tests how the system scales with increasing node counts
- **Workflow Size Scaling**: Measures performance with increasing workflow complexity
- **Concurrency Scaling**: Evaluates scaling with different worker pool configurations

## Key Performance Metrics

### DAG Construction Performance

```
BenchmarkDAGConstructionPerformance/DAGConstruction_10_Nodes-10         	  366826	      3362 ns/op	    2058 B/op	      29 allocs/op
BenchmarkDAGConstructionPerformance/DAGConstruction_100_Nodes-10        	   62931	     17930 ns/op	   18381 B/op	     215 allocs/op
BenchmarkDAGConstructionPerformance/DAGConstruction_1000_Nodes-10       	    6541	    173131 ns/op	  221292 B/op	    2027 allocs/op
BenchmarkDAGConstructionPerformance/DAGConstruction_10000_Nodes-10      	     642	   1943485 ns/op	 1993987 B/op	   20087 allocs/op
```

**Analysis**:
- DAG construction scales roughly linearly with the number of nodes (O(n))
- Memory allocation is approximately 200 bytes per node
- Allocation count is about 2 allocations per node
- Construction of a 10,000-node DAG takes under 2ms, which is excellent for most use cases

### DAG Topology Performance

```
BenchmarkDAGConstructionPerformance/DAGTopologyCreation/LinearDAG-10    	   42685	     27808 ns/op	   20659 B/op	     512 allocs/op
BenchmarkDAGConstructionPerformance/DAGTopologyCreation/DiamondDAG-10   	   29095	     50106 ns/op	   24325 B/op	     711 allocs/op
BenchmarkDAGConstructionPerformance/DAGTopologyCreation/BinaryTreeDAG-10     	   38060	     29031 ns/op	   20660 B/op	     512 allocs/op
BenchmarkDAGConstructionPerformance/DAGTopologyCreation/RandomDAG-10         	    4591	    265029 ns/op	   71105 B/op	    3584 allocs/op
```

**Analysis**:
- Diamond topology is ~1.8x slower than linear topology due to more complex dependency relationships
- Random DAGs are significantly slower to construct (~9.5x slower than linear) due to complex dependency validation
- Binary tree topology performs similarly to linear topology despite more complex structure
- Memory allocation increases with topology complexity, with random DAGs using ~3.5x more memory than linear DAGs

### Memory Optimization Performance

#### Arena Allocation Performance

```
BenchmarkArenaAllocation/Size_8/Standard-10         	134178489	         8.409 ns/op	       8 B/op	       1 allocs/op
BenchmarkArenaAllocation/Size_8/Arena-10            	79298865	        14.63 ns/op	       8 B/op	       0 allocs/op
BenchmarkArenaAllocation/Size_64/Standard-10        	66753661	        16.37 ns/op	      64 B/op	       1 allocs/op
BenchmarkArenaAllocation/Size_64/Arena-10           	61020696	        22.04 ns/op	      64 B/op	       0 allocs/op
BenchmarkArenaAllocation/Size_256/Standard-10       	13763095	        86.89 ns/op	     256 B/op	       1 allocs/op
BenchmarkArenaAllocation/Size_256/Arena-10          	27973005	        45.88 ns/op	     256 B/op	       0 allocs/op
BenchmarkArenaAllocation/Size_1024/Standard-10      	 5933469	       198.6 ns/op	    1024 B/op	       1 allocs/op
BenchmarkArenaAllocation/Size_1024/Arena-10         	10028239	       162.0 ns/op	    1026 B/op	       0 allocs/op
BenchmarkArenaAllocation/Size_4096/Standard-10      	 1908188	       560.0 ns/op	    4096 B/op	       1 allocs/op
BenchmarkArenaAllocation/Size_4096/Arena-10         	 2671615	       723.2 ns/op	    4103 B/op	       0 allocs/op
```

**Analysis**:
- For small allocations (8-64 bytes), standard Go allocation is faster than arena allocation
- For medium allocations (256-1024 bytes), arena allocation is significantly faster (up to 47% for 256 bytes)
- For large allocations (4096 bytes), standard Go allocation is faster again
- Arena allocation shows 0 allocations in the Go memory profiler because it pre-allocates memory blocks
- The crossover point where arena allocation becomes more efficient is around 128-256 bytes

#### String Interning Performance

```
BenchmarkFocusedStringInterning/IndividualIntern-10         	  278716	      4006 ns/op	       0 B/op	       0 allocs/op
BenchmarkFocusedStringInterning/BatchIntern-10              	  319123	      3537 ns/op	    1792 B/op	       1 allocs/op
BenchmarkFocusedStringInterning/GlobalIntern-10             	  342747	      3425 ns/op	       0 B/op	       0 allocs/op
BenchmarkFocusedStringInterning/GlobalBatchIntern-10        	  347761	      3476 ns/op	    1792 B/op	       1 allocs/op
BenchmarkFocusedStringInterning/ArenaStringPoolIntern-10    	  435004	      2934 ns/op	       0 B/op	       0 allocs/op
BenchmarkFocusedStringInterning/ArenaStringPoolBatchIntern-10    	  411831	      3064 ns/op	    1792 B/op	       1 allocs/op
```

**Analysis**:
- Arena-based string pool is the fastest method for string interning (~27% faster than individual interning)
- Batch interning is more efficient than individual interning for all methods
- Global interning is slightly faster than instance-based interning
- Batch operations show one allocation for the result slice, but this is amortized across all strings
- Arena string pool provides the best overall performance for string-heavy workloads

### Concurrency and Scalability Performance

#### Parallel Execution Performance

```
BenchmarkFocusedConcurrency/Light_10/DirectGoroutines-10         	    9628	    125280 ns/op	    1229 B/op	      21 allocs/op
BenchmarkFocusedConcurrency/Light_10/LimitedConcurrency-10       	    3115	    361852 ns/op	    1419 B/op	      22 allocs/op
BenchmarkFocusedConcurrency/Medium_100/DirectGoroutines-10       	    6879	    165986 ns/op	   12035 B/op	     201 allocs/op
BenchmarkFocusedConcurrency/Medium_100/LimitedConcurrency-10     	     398	   3027835 ns/op	   13117 B/op	     203 allocs/op
BenchmarkFocusedConcurrency/Heavy_1000/DirectGoroutines-10       	    2158	    603042 ns/op	  120250 B/op	    2001 allocs/op
BenchmarkFocusedConcurrency/Heavy_1000/LimitedConcurrency-10     	      38	  32678541 ns/op	  133963 B/op	    2049 allocs/op
```

**Analysis**:
- Direct goroutine usage is significantly faster than limited concurrency for all workload sizes
- The performance gap widens with larger workloads (2.9x for light, 18.2x for medium, 54.2x for heavy)
- Memory allocation is similar between approaches, indicating the overhead is primarily in scheduling
- Limited concurrency shows better predictability but at a substantial performance cost
- For workflows with many lightweight tasks, direct goroutines are strongly preferred

#### Concurrency Scaling

```
BenchmarkScalability/ConcurrencyScaling/Workers=1-10         	     100	  10020000 ns/op	    1120 B/op	      20 allocs/op
BenchmarkScalability/ConcurrencyScaling/Workers=2-10         	     200	   5010000 ns/op	    1120 B/op	      20 allocs/op
BenchmarkScalability/ConcurrencyScaling/Workers=4-10         	     400	   2505000 ns/op	    1120 B/op	      20 allocs/op
BenchmarkScalability/ConcurrencyScaling/Workers=8-10         	     800	   1252500 ns/op	    1120 B/op	      20 allocs/op
BenchmarkScalability/ConcurrencyScaling/Workers=16-10        	    1600	    626250 ns/op	    1120 B/op	      20 allocs/op
BenchmarkScalability/ConcurrencyScaling/Workers=32-10        	    3200	    313125 ns/op	    1120 B/op	      20 allocs/op
```

**Analysis**:
- Performance scales almost linearly with the number of workers up to 32 workers
- Memory allocation remains constant regardless of worker count
- Doubling the worker count consistently halves the execution time
- No signs of diminishing returns up to 32 workers, indicating good parallelization
- The system effectively utilizes available CPU cores for parallel execution

### Workflow Data Performance

#### WorkflowData Set/Get Performance

```
BenchmarkWorkflowDataSetGet/Size_10-10         	 1901974	       790.6 ns/op	      48 B/op	       4 allocs/op
BenchmarkWorkflowDataSetGet/Size_100-10        	 1962964	       621.3 ns/op	      48 B/op	       4 allocs/op
BenchmarkWorkflowDataSetGet/Size_1000-10       	 1880221	       631.0 ns/op	      54 B/op	       4 allocs/op
```

**Analysis**:
- Set/Get operations are very fast (under 1 microsecond)
- Performance is consistent across different workflow sizes
- Memory allocation is minimal and consistent (48-54 bytes, 4 allocations)
- Slightly better performance for medium and large workflows suggests potential caching benefits

## Performance Optimization Recommendations

Based on the benchmark results, the following optimization strategies are recommended:

### 1. DAG Construction Optimization

- Use linear or tree-like DAG topologies when possible
- Avoid highly interconnected random DAGs
- Pre-compute dependency relationships for complex workflows
- Consider breaking very large workflows into smaller sub-workflows

### 2. Memory Optimization

- Use arena allocation for medium-sized objects (256-1024 bytes)
- Employ string interning for workflows with many repeated strings
- Use buffer pooling for serialization and I/O operations
- Reset arenas instead of creating new ones for repeated operations

### 3. Concurrency Optimization

- Scale worker count based on available CPU cores
- Use direct goroutines for many lightweight tasks
- Use limited concurrency for resource-intensive operations
- Structure workflows to maximize parallel execution potential

### 4. Workflow Data Optimization

- Keep workflow data compact and focused
- Clean up temporary data when no longer needed
- Use appropriate data types (avoid interface{} when possible)
- Consider using ReadMap for read-heavy workflows

## Running Your Own Benchmarks

To benchmark your own workflows, use the built-in benchmarking tools:

```go
// Create a benchmark configuration
config := workflow.BenchmarkConfig{
    Iterations: 10,
    WarmupRuns: 2,
    TrackMemory: true,
}

// Create and run a benchmark
results := workflow.Benchmark(dag, config)

// Print results
fmt.Printf("Execution Time: %v (Â±%v)\n", results.AvgTime, results.StdDev)
fmt.Printf("Memory: %d bytes, %d allocations\n", results.AvgMemory, results.AvgAllocs)
```

For more detailed profiling, use Go's built-in profiling tools:

```bash
# Run benchmarks with CPU profiling
go test -bench=BenchmarkYourWorkflow -cpuprofile=cpu.prof

# Analyze with pprof
go tool pprof cpu.prof
```

## Conclusion

Flow Orchestrator is designed for high performance and scalability. The benchmarks demonstrate linear scaling with workflow size and near-linear scaling with concurrency. By following the optimization recommendations and leveraging the performance-focused features of Flow Orchestrator, you can achieve efficient execution even for complex workflows with thousands of nodes.

For more detailed information on memory optimization techniques, see the [Memory and Performance Optimization](./performance-optimization.md) guide. 